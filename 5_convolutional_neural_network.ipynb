{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN and filters \n",
    "\n",
    "### Element-wise multiplication\n",
    "\n",
    "element-wise multiplication between the filter-sized patch of the input and filter, which is then summed, always resulting in a single value. This filter has to be design to detect specific type of feature in image, in other words specific pattern on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a static matrix representing the number 9\n",
    "number_9 = np.array([\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 1, 1, 1, 0]\n",
    "])\n",
    "\n",
    "# Define a simple vertical edge detection filter. Specific type of filter designed to detect vertical edges in images. \n",
    "#The first column (-1, -1, -1) is designed to respond strongly to pixels on one side of a vertical edge.\n",
    "#The second column (0, 0, 0) is neutral, providing no response to uniform areas.\n",
    "#The third column (1, 1, 1) responds strongly to pixels on the opposite side of a vertical edge.\n",
    "vertical_edge_filter = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "# Function to perform the convolution operation without padding\n",
    "\n",
    "def convolve2d_no_padding(image, kernel):\n",
    "    \"\"\"_summary_\n",
    "    When this filter is convolved over an image, it calculates the difference in pixel \n",
    "    intensity between the left and right sides of the area under the filter.\n",
    "    \n",
    "    If there is a vertical edge in the image, there will be a significant difference in intensity between the left and right sides of the edge.\n",
    "    This difference results in a high value in the feature map at the edge's location.\n",
    "    \n",
    "    The -1 values enhance the detection of one side of the edge, while the 1 values enhance the other side, \n",
    "    making the filter sensitive to vertical transitions in pixel intensity from left to right.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculating the size of the output based on the image and kernel sizes\n",
    "    # The output size is smaller than the input image size due to lack of padding\n",
    "    output_size = (\n",
    "        image.shape[0] - kernel.shape[0] + 1, \n",
    "        image.shape[1] - kernel.shape[1] + 1\n",
    "    )\n",
    "    \n",
    "    output = np.zeros(output_size)  # Output array with reduced size\n",
    "\n",
    "    for x in range(output_size[1]):\n",
    "        for y in range(output_size[0]):\n",
    "            # Applying the convolution operation:\n",
    "            # Element-wise multiplication of the kernel with the corresponding image region\n",
    "            # and summing the results to get a single value         \n",
    "                       \n",
    "            #kernel or filter:            \n",
    "            #array([[-1,  0,  1],\n",
    "            #       [-1,  0,  1],\n",
    "            #       [-1,  0,  1]])\n",
    "                      \n",
    "            #image[y:y+3, x:x+3] or patch:\n",
    "            #array([[0, 1, 1],\n",
    "            #       [0, 1, 0],\n",
    "            #       [0, 1, 1]])\n",
    "\n",
    "            # use ELEMENT-WISE multiplication (denoted by * in NumPy) instead of matrix multiplication                           \n",
    "            # kernel * image[y:y+3, x:x+3])  \n",
    "            #array([[0, 0, 1],\n",
    "            #       [0, 0, 0],\n",
    "            #       [0, 0, 1]])\n",
    "            \n",
    "            \n",
    "            #(kernel * image[y:y+3, x:x+3]).sum() = 2            \n",
    "            output[y, x] = (kernel * image[y:y+3, x:x+3]).sum()\n",
    "\n",
    "    return output\n",
    "\n",
    "def plot_filter_and_feature_map(number_9, vertical_edge_filter, convolved_image_no_padding):\n",
    "    # Display the matrices along with their values in each cell\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # Original number 9 matrix\n",
    "    axs[0].set_title(f\"Original Image (Number 9 Matrix)\\nShape: {number_9.shape}\")\n",
    "    axs[0].imshow(number_9, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(number_9):\n",
    "        axs[0].text(i, j, int(val), ha='center', va='center', color='red')\n",
    "\n",
    "    # Vertical edge detection filter matrix\n",
    "    axs[1].set_title(f\"Vertical Edge Detection Filter\\nShape: {vertical_edge_filter.shape}\")\n",
    "    axs[1].imshow(vertical_edge_filter, cmap='gray', interpolation='none')\n",
    "    for (j, i), val in np.ndenumerate(vertical_edge_filter):\n",
    "        axs[1].text(i, j, int(val), ha='center', va='center', color='red')\n",
    "\n",
    "    # Convolved image (without padding)\n",
    "    axs[2].set_title(f\"Convolved Image (No Padding)\\nShape: {convolved_image_no_padding.shape}\")\n",
    "    axs[2].imshow(convolved_image_no_padding, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(convolved_image_no_padding):\n",
    "        axs[2].text(i, j, f\"{val:.1f}\", ha='center', va='center', color='red')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the relevant matrices\n",
    "convolved_image_no_padding = convolve2d_no_padding(image=number_9, kernel=vertical_edge_filter)\n",
    "plot_filter_and_feature_map(number_9, vertical_edge_filter, convolved_image_no_padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters normalization\n",
    "\n",
    "- **Consistent Range of Feature Values:** Useful for handling filters of varying sizes or scales.\n",
    "\n",
    "- **Improved Training Stability:** Helps prevent excessively large values in feature maps, reducing the risk of exploding gradients.\n",
    "\n",
    "- **Balancing Contributions of Different Filters:** Ensures that both large and small filters contribute equally to the feature map.\n",
    "\n",
    "- **Better Generalization:** Prevents filters from overly focusing on specific training data details, like line thickness or digit brightness.\n",
    "\n",
    "- **Mitigating Overfitting:** Aids in reducing overfitting, contributing to more stable feature map values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2d_normalize_no_padding(image, kernel):\n",
    "    output_size = (\n",
    "        image.shape[0] - kernel.shape[0] + 1, \n",
    "        image.shape[1] - kernel.shape[1] + 1\n",
    "    )\n",
    "    \n",
    "    output_convoluted = np.zeros(output_size)  # Output array with reduced size\n",
    "    print(\"Initial image:\")\n",
    "    print(number_9)\n",
    "\n",
    "    #print(\"Kernel * image window multiplication:\") \n",
    "    for x in range(output_size[1]):\n",
    "        print(\" ### Step \",x + 1)\n",
    "        for y in range(output_size[0]):    \n",
    "            #Apply filter with Element-Wise multiplication\n",
    "            output_convoluted[y, x] = ((kernel * image[y:y+3, x:x+3]).sum()) / (kernel.shape[0] * kernel.shape[1]) \n",
    "            \n",
    "            #Print\n",
    "            kernel_str = '\\n'.join([' '.join([f\"{item: .2f}\" for item in row]) for row in kernel])\n",
    "            image_window_str = '\\n'.join([' '.join([f\"{item: .2f}\" for item in row]) for row in image[y:y+3, x:x+3]])\n",
    "            convoluted_img_str = '\\n'.join([' '.join([f\"{item: .2f}\" for item in row]) for row in output_convoluted])\n",
    "            print(f\"Kernel:\\n{kernel_str}\\nImage window:\\n{image_window_str}\\nConvoluted Image:\\n{convoluted_img_str}\")\n",
    "\n",
    "            \n",
    "    return output_convoluted\n",
    "\n",
    "convolved_image_no_padding = convolve2d_normalize_no_padding(image=number_9, kernel=vertical_edge_filter)\n",
    "plot_filter_and_feature_map(number_9, vertical_edge_filter, convolved_image_no_padding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Pooling \n",
    "\n",
    "- **Function:** Selects the maximum value from a feature map within a specified window (pooling size).\n",
    "\n",
    "- **Purpose:** Emphasizes the most prominent features in a specific area of the input.\n",
    "\n",
    "- **Effect on Data:** Reduces the spatial dimensions (width and height) of the input feature map, making the representation smaller and more manageable.\n",
    "\n",
    "- **Impact on Model:** Helps in making the detection of features invariant to scale and orientation changes.\n",
    "\n",
    "- **Use Cases:** Often used in models where detection of high-intensity features like edges is crucial.\n",
    "\n",
    "- **Overfitting:** Helps in reducing overfitting by providing an abstracted form of the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Existing code ...\n",
    "\n",
    "# Define the max pooling function\n",
    "def max_pooling(input_matrix, pool_size):\n",
    "    # Calculate the shape of the output matrix\n",
    "    output_shape = (\n",
    "        input_matrix.shape[0] // pool_size,\n",
    "        input_matrix.shape[1] // pool_size\n",
    "    )\n",
    "\n",
    "    # Initialize the output matrix with zeros\n",
    "    output_matrix = np.zeros(output_shape)\n",
    "\n",
    "    # Apply max pooling with proper boundary checks\n",
    "    for y in range(output_shape[0]):\n",
    "        for x in range(output_shape[1]):\n",
    "            # Defining the region for pooling\n",
    "            start_y, end_y = y * pool_size, (y + 1) * pool_size\n",
    "            start_x, end_x = x * pool_size, (x + 1) * pool_size\n",
    "            \n",
    "            # Apply pooling operation within the defined region\n",
    "            output_matrix[y, x] = np.max(input_matrix[start_y:end_y, start_x:end_x])\n",
    "\n",
    "    return output_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Define the plot_filter_feature_map_average_pooling function\n",
    "def plot_filter_feature_map_max_pooling(number_9, vertical_edge_filter, convolved_image, pooled_image):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # Adjusted to 4 subplots\n",
    "\n",
    "    # Original image\n",
    "    axs[0].set_title(\"Original Image (Number 9 Matrix)\")\n",
    "    axs[0].imshow(number_9, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(number_9):\n",
    "        axs[0].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Vertical edge detection filter matrix\n",
    "    axs[1].set_title(\"Vertical Edge Detection Filter\")\n",
    "    axs[1].imshow(vertical_edge_filter, cmap='gray', interpolation='none')\n",
    "    for (j, i), val in np.ndenumerate(vertical_edge_filter):\n",
    "        axs[1].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Convolved image\n",
    "    axs[2].set_title(\"Convolved Image\")\n",
    "    axs[2].imshow(convolved_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(convolved_image):\n",
    "        axs[2].text(i, j, f\"{val:.2f}\", ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Max pooled image\n",
    "    axs[3].set_title(\"Max Pooled Image\")\n",
    "    axs[3].imshow(pooled_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(pooled_image):\n",
    "        axs[3].text(i, j, f\"{val:.2f}\", ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Apply convolution and max pooling\n",
    "convolved_image_no_padding = convolve2d_no_padding(number_9, vertical_edge_filter)\n",
    "max_pooled_image = max_pooling(convolved_image_no_padding, pool_size=2)\n",
    "\n",
    "# Call the plotting function\n",
    "plot_filter_feature_map_max_pooling(number_9, vertical_edge_filter, convolved_image_no_padding, max_pooled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Pooling\n",
    "- **Function:** Calculates the average of the values in a feature map within a specified window (pooling size).\n",
    "\n",
    "- **Purpose:** Smooths out the image and feature map, focusing on the overall structure rather than specific details.\n",
    "\n",
    "- **Effect on Data:** Similar to max pooling, it reduces the spatial dimensions but by taking the average, it retains more background information.\n",
    "\n",
    "- **Impact on Model:** Less aggressive compared to max pooling, retaining more information which can be useful or excessive.\n",
    "\n",
    "- **Use Cases:** Beneficial in cases where the background and overall texture information are important.\n",
    "\n",
    "- **Overfitting:** Can potentially retain too much information, leading to less abstract feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pooling(input_matrix, pool_size):\n",
    "    \"\"\"\n",
    "    This function applies average pooling to the input matrix.\n",
    "\n",
    "    The pool_size parameter defines the size of the pooling window.\n",
    "    The function iterates over the input matrix in steps equal to the pool size and\n",
    "    calculates the average value within each pooling window.\n",
    "\n",
    "    The result is a smaller matrix where each element is the average value of a\n",
    "    sub-region in the original matrix.\n",
    "    \"\"\"\n",
    "    # Calculate the shape of the output matrix\n",
    "    output_shape = (\n",
    "        input_matrix.shape[0] // pool_size,\n",
    "        input_matrix.shape[1] // pool_size\n",
    "    )\n",
    "\n",
    "    # Initialize the output matrix with zeros\n",
    "    output_matrix = np.zeros(output_shape)\n",
    "\n",
    "    # Apply average pooling with proper boundary checks\n",
    "    for y in range(output_shape[0]):\n",
    "        for x in range(output_shape[1]):\n",
    "            # Defining the region for pooling\n",
    "            start_y, end_y = y * pool_size, (y + 1) * pool_size\n",
    "            start_x, end_x = x * pool_size, (x + 1) * pool_size\n",
    "            \n",
    "            # Apply pooling operation within the defined region\n",
    "            output_matrix[y, x] = np.mean(input_matrix[start_y:end_y, start_x:end_x])\n",
    "\n",
    "    return output_matrix\n",
    "\n",
    "# Define the plot_filter_feature_map_average_pooling function\n",
    "def plot_filter_feature_map_average_pooling(number_9, vertical_edge_filter, convolved_image, pooled_image):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # Adjusted to 4 subplots\n",
    "\n",
    "    # Original image\n",
    "    axs[0].set_title(\"Original Image (Number 9 Matrix)\")\n",
    "    axs[0].imshow(number_9, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(number_9):\n",
    "        axs[0].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Vertical edge detection filter matrix\n",
    "    axs[1].set_title(\"Vertical Edge Detection Filter\")\n",
    "    axs[1].imshow(vertical_edge_filter, cmap='gray', interpolation='none')\n",
    "    for (j, i), val in np.ndenumerate(vertical_edge_filter):\n",
    "        axs[1].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Convolved image\n",
    "    axs[2].set_title(\"Convolved Image\")\n",
    "    axs[2].imshow(convolved_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(convolved_image):\n",
    "        axs[2].text(i, j, f\"{val:.2f}\", ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Max pooled image\n",
    "    axs[3].set_title(\"Average Pooled Image\")\n",
    "    axs[3].imshow(pooled_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(pooled_image):\n",
    "        axs[3].text(i, j, f\"{val:.2f}\", ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Step 2: Perform Convolution\n",
    "convolved_image = convolve2d_no_padding(number_9, vertical_edge_filter)\n",
    "\n",
    "# Step 3: Perform Max Pooling\n",
    "average_pooled_image = average_pooling(convolved_image, pool_size=2)\n",
    "\n",
    "# Call the plotting function\n",
    "plot_filter_feature_map_average_pooling(number_9, vertical_edge_filter, convolved_image_no_padding, average_pooled_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "In convolutional neural networks (CNNs), padding is typically applied to the original image (or the input feature map at each layer), not to the filter. \n",
    "- **Edge Information Preservation:** By padding the input image, you ensure that the convolution operation captures the information at the edges of the image. This is particularly important because the pixels on the edges and corners are otherwise less utilized in the absence of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the padding function\n",
    "def padding(input_matrix, pad_size):\n",
    "    \"\"\"\n",
    "    Adds zero padding to the input matrix.\n",
    "\n",
    "    Args:\n",
    "        input_matrix (numpy.ndarray): The input matrix to be padded.\n",
    "        pad_size (int): The size of the padding.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Padded matrix.\n",
    "    \"\"\"\n",
    "    return np.pad(input_matrix, ((pad_size, pad_size), (pad_size, pad_size)), mode='constant', constant_values=0)\n",
    "\n",
    "# Define the plot_filter_feature_map_average_pooling function\n",
    "def plot_filter_feature_map_average_pooling(number_9, vertical_edge_filter, padded_image, convolved_image, pooled_image):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(25, 5))  # Adjusted to 5 subplots\n",
    "\n",
    "    # Original image\n",
    "    axs[0].set_title(\"Original Image (Number 9 Matrix)\")\n",
    "    axs[0].imshow(number_9, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(number_9):\n",
    "        axs[0].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Vertical edge detection filter matrix\n",
    "    axs[1].set_title(\"Vertical Edge Detection Filter\")\n",
    "    axs[1].imshow(vertical_edge_filter, cmap='gray', interpolation='none')\n",
    "    for (j, i), val in np.ndenumerate(vertical_edge_filter):\n",
    "        axs[1].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Padded image\n",
    "    axs[2].set_title(\"Padded Image\")\n",
    "    axs[2].imshow(padded_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(padded_image):\n",
    "        axs[2].text(i, j, int(val), ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Convolved image\n",
    "    axs[3].set_title(\"Convolved Image\")\n",
    "    axs[3].imshow(convolved_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(convolved_image):\n",
    "        axs[3].text(i, j, f\"{val:.2f}\", ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    # Average pooled image\n",
    "    axs[4].set_title(\"Average Pooled Image\")\n",
    "    axs[4].imshow(pooled_image, cmap='gray')\n",
    "    for (j, i), val in np.ndenumerate(pooled_image):\n",
    "        axs[4].text(i, j, f\"{val:.2f}\", ha='center', va='center', color='red', fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Assuming number_9, vertical_edge_filter, convolve2d_no_padding,\n",
    "# average_pooling are already defined\n",
    "\n",
    "# Apply padding\n",
    "pad_size = 1  # Adjust pad size as needed\n",
    "padded_number_9 = padding(number_9, pad_size)\n",
    "\n",
    "# Perform Convolution with Padding\n",
    "convolved_image_with_padding = convolve2d_no_padding(padded_number_9, vertical_edge_filter)\n",
    "\n",
    "# Perform Average Pooling\n",
    "average_pooled_image = average_pooling(convolved_image_with_padding, pool_size=2)\n",
    "\n",
    "# Call the plotting function\n",
    "plot_filter_feature_map_average_pooling(number_9, vertical_edge_filter, padded_number_9, convolved_image_with_padding, average_pooled_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But what is a convolution?\n",
    "- https://www.youtube.com/watch?v=KuXjwB4LzSA&ab_channel=3Blue1Brown\n",
    "\n",
    "- Convolution is a mathematical operation commonly used in **signal processing, image analysis, probability theory**, and various other fields.\n",
    "\n",
    "- The process of convolution is **integral to systems that perform signal filtering**, analysis, and transformation.\n",
    "\n",
    "- It **combines two functions to produce a third function** that expresses **how the shape of one is modified by the other**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conv1d(x, w, p=0, s=1):\n",
    "    w_rot = np.array(w[::-1])\n",
    "    x_padded = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int(len(x)/s), s):\n",
    "        res.append(np.sum(x_padded[i:i+w_rot.shape[0]] * w_rot))\n",
    "    return np.array(res)\n",
    "\n",
    "## Testing:\n",
    "x = [1, 3, 2, 4, 5]\n",
    "w = [1, 0, 3, 1, 2]\n",
    "\n",
    "print('Conv1d Implementation: ',\n",
    "      conv1d(x, w, p=2, s=1))\n",
    "print('Numpy Results:         ',\n",
    "      np.convolve(x, w, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Example\n",
    "\n",
    "### Application: COVID Ventilator Usage\n",
    "\n",
    "- Set as the percent of patients needing ventilators. For example, F = [.05, .03, .01] means 5% of patients need ventilators the **first week**, 3% the **second week**, and 1% the **third week.**\n",
    "\n",
    "- Set as the weekly incoming patients, in thousands. G = [10, 20, 30, 20, 10, 10, 10]\n",
    "\n",
    "- The convolution, shows how many ventilators are needed each week (in thousands). \n",
    " is how many ventilators are needed 5 weeks from now.\n",
    "\n",
    "See details https://betterexplained.com/articles/intuitive-convolution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = [3, 2, 1]\n",
    "g = [1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "def convoluve(data, filter1) -> list:\n",
    "    convoluted_result = []\n",
    "\n",
    "    # Length of the convoluted result\n",
    "    result_length = len(data) + len(filter1) - 1\n",
    "\n",
    "    # Convolution process\n",
    "    for d in range(result_length):\n",
    "        sum_conv = 0\n",
    "        for f in range(len(filter1)):\n",
    "            # Calculate the index in the data array\n",
    "            data_index = d - f\n",
    "            \n",
    "            # Check if the index is within the bounds of the data array\n",
    "            if 0 <= data_index < len(data):\n",
    "                sum_conv += filter1[f] * data[data_index]\n",
    "        \n",
    "        convoluted_result.append(sum_conv)\n",
    "\n",
    "    return convoluted_result\n",
    "            \n",
    "            \n",
    "    \n",
    "# Let's call our treatment plan  f(x) In our example, we used [3 2 1].\n",
    "conv_res = np.convolve(f, g)\n",
    "conv_res2 = convoluve(g,f)\n",
    "print(conv_res)\n",
    "print(conv_res2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Filtered Example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "\n",
    "def dislay_img(img, title):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Step 1: Read the image\n",
    "#img = cv2.imread('./img/5/platonic_solids_before_plato.png')\n",
    "img_RGB = cv2.imread('./img/5/platonic_solids.png')\n",
    "print(img_RGB.shape)\n",
    "\n",
    "img_gray = cv2.cvtColor(img_RGB, cv2.COLOR_BGR2GRAY)\n",
    "print(img_gray.shape)\n",
    "print(img_gray[100, 100])\n",
    "dislay_img(img_gray, 'Original gray')\n",
    "\n",
    "\n",
    "def add_noise(img):    \n",
    "    # Step 1: Create Gaussian noise\n",
    "    mean = 0\n",
    "    std = 10  # Standard deviation of the noise\n",
    "    gaussian_noise = np.random.normal(mean, std, img.shape).astype(np.uint8)\n",
    "    # Step 2: Add the noise to the image\n",
    "    noisy_image = cv2.add(img_gray, gaussian_noise)\n",
    "    # Step 4: Display or save the noisy image\n",
    "    # Display the image\n",
    "    return noisy_image\n",
    "    \n",
    "\n",
    "\n",
    "#Filters\n",
    "# Gaussian Filter (3x3)\n",
    "gaussian_filter = np.array([[1, 2, 1],\n",
    "                            [2, 4, 2],\n",
    "                            [1, 2, 1]]) / 16\n",
    "\n",
    "filter_vertical_edges = np.array([[1, 0, -1],\n",
    "                                  [2, 0, -2],\n",
    "                                  [1, 0, -1]])\n",
    "\n",
    "filter_horizontal_edges = np.array([[1, 2, 1],\n",
    "                                    [0, 0, 0],\n",
    "                                    [-1, -2, -1]])\n",
    "\n",
    "filter_s4 = np.array([[-1, -1, -1],\n",
    "                      [-1, 8, -1],\n",
    "                     [ -1, -1, -1]])\n",
    "\n",
    "filter1_s5 = np.array([[0, 0, 0],\n",
    "                       [1, 0, -1],\n",
    "                       [0, 0, 0]])\n",
    "\n",
    "#Apply filters \n",
    "img_noised = add_noise(img_gray)\n",
    "dislay_img(img_noised, \"Noisy image\")\n",
    "\n",
    "img_noise_filtered = convolve2d(img_noised, gaussian_filter, mode='same')\n",
    "dislay_img(img_noise_filtered, \"Noisy filtered\")\n",
    "\n",
    "img_vert_edges = convolve2d(img_gray, filter_vertical_edges, mode='same')\n",
    "dislay_img(img_vert_edges, \"Vertical edges\")\n",
    "\n",
    "img_hor_edges = convolve2d(img_gray, filter_horizontal_edges, mode='same')\n",
    "dislay_img(img_hor_edges, \"Horizontal edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN example with PyTorch\n",
    "\n",
    "### Understanding Filters in Convolutional Layers\n",
    "In PyTorch, when you create a Conv2d layer, you specify the number of input channels, the number of output channels (filters), and the size of each filter. For instance:\n",
    "\n",
    "self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "\n",
    "- in_channels=1: Number of input channels (e.g., 1 for grayscale images, 3 for RGB images).\n",
    "- out_channels=32: Number of filters. This layer will have 32 filters.\n",
    "- kernel_size=5: Size of each filter. Each filter will be 5x5 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.5158190844155578\n",
      "Epoch 2/2, Loss: 0.3300538538361409\n",
      "Finished Training\n",
      "Test Loss: 0.36539909699160583\n",
      "Accuracy: 86.83%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.Grayscale(num_output_channels=3),  # Convert 1-channel images to 3-channel\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "# ])\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "num_classes = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=5)  # Change to 1 input channel\n",
    "        self.conv2 = nn.Conv2d(28, 64, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Dummy forward pass to calculate the size of the flattened features\n",
    "        x = torch.randn(1, 1, 28, 28)  # Adjust for 1-channel input\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        self.flattened_size = x.view(-1).shape[0]\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Using CIFAR-10 dataset\n",
    "#train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_dataloader:\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimize the weights\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print statistics after every epoch\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_dataloader)}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "def test_model(model, test_dataloader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # No gradient tracking needed\n",
    "        for images, labels in test_dataloader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print test results\n",
    "    print(f'Test Loss: {running_loss / len(test_dataloader)}')\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n",
    "    \n",
    "# Initialize the Model, Loss Function, and Optimizer\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Train the model\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs=2)\n",
    "# Test the model\n",
    "test_model(model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Convolutional Neural Networks\n",
    "\n",
    " also serve as basic feature generators for more advanced tasks such as tracking (Zhang et al., 2021), segmentation (Long et al., 2015), object detection (Redmon and Farhadi, 2018), or style transformation (Gatys et al., 2016). \n",
    "\n",
    "\n",
    "Next will be presented models in chronological order, partly to convey a sense of the history so that you can form your own intuitions about where the field is heading and perhaps develop your own architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet \n",
    "\n",
    "Is a significant convolutional neural network (CNN) architecture that played a pivotal role in the field of deep learning, particularly in computer vision. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, it was introduced in 2012 in a paper titled \"ImageNet Classification with Deep Convolutional Neural Networks\". Here are the main ideas and features of AlexNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network in Network (NiN)\n",
    "\n",
    "- Replacing Conventional Layers with MLPConv: NiN's key idea was to replace the standard convolutional layers with 'mlpconv' layers. These layers are a combination of a convolution followed by one or more 1x1 convolutions, which can be seen as a mini fully connected network applied at every pixel location.\n",
    "- Enhanced Feature Abstraction: The 1x1 convolutions (mini MLPs) enable the network to learn more complex and abstract representations at each layer, moving beyond just spatial aggregations performed by standard convolutions  \n",
    "- Global Average Pooling: NiN used global average pooling at the end of the network instead of fully connected layers, which significantly reduced the number of parameters and helped mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(NiN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.nin_block1 = self.nin_block(3, 192, kernel_size=5, stride=1, padding=2)\n",
    "        self.nin_block2 = self.nin_block(192, 160, kernel_size=5, stride=1, padding=2)\n",
    "        self.nin_block3 = self.nin_block(160, 96, kernel_size=3, stride=1, padding=1)\n",
    "        self.nin_block4 = self.nin_block(96, num_classes, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def nin_block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        return block\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.nin_block1(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        x = self.nin_block2(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        x = self.nin_block3(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        x = self.nin_block4(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), self.num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks (ResNet) and ResNeXt\n",
    "\n",
    "- Block Design: effectively create multiple paths within each block. This is known as the split-transform-merge strategy. Let's break down this concept with a simplified example.\n",
    "\n",
    "### Split-Transform-Merge strategy:\n",
    "- Split: The input to the block is split into multiple smaller sets of feature maps. This can be achieved through grouped convolutions, where the input channels are divided into groups.\t\n",
    "- Transform: Each group of feature maps is then processed by a set of convolutions (or other transformations). In ResNeXt, these are typically a series of 1x1, 3x3, and again 1x1 convolutions, similar to a bottleneck design in ResNet but applied to each split independently.\n",
    "- Merge: The outputs of these parallel transformations are then concatenated or summed together before being passed to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, cardinality, bottleneck_width):\n",
    "        super(ResNeXtBlock, self).__init__()\n",
    "        D = cardinality * bottleneck_width\n",
    "        self.conv1 = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=1, padding=1, groups=cardinality)\n",
    "        self.conv3 = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x += shortcut\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
