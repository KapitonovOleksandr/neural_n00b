{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent for simple Linear Regression with ordinary derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Given data points\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "n = len(x)\n",
    "\n",
    "# Function to calculate MSE\n",
    "def calculate_mse(w, x, y):\n",
    "    \"\"\"\n",
    "    Compute gradient: \n",
    "    The value of the MSE is informative and is often used to monitor the performance and convergence of the model during training.\n",
    "    Calculate the gradient of the cost function (which is the MSE in this context) with respect to the weights. \n",
    "    This tells us how much the cost would change if we made a small change to the weight.\n",
    "    \"\"\"\n",
    "    errors = (w * x - y) ** 2\n",
    "    mse = errors.mean()\n",
    "    return mse\n",
    "\n",
    "# Function to calculate the gradient of the cost function with respect to 'w'\n",
    "def calculate_gradient(w, x, y):\n",
    "    \"\"\"\n",
    "    Update weights: \n",
    "    In gradient descent, the weights are updated based on the gradient of the cost function, NOT directly on the cost function value (MSE in this case).\n",
    "    Adjust the weights by a small step in the opposite direction of the gradient. \n",
    "    This is because the gradient points in the direction of the steepest increase of the cost function, and we want to minimize the cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the errors for each data point\n",
    "    errors = w * x - y\n",
    "    \n",
    "    # Multiply the errors by the input (x) to get the gradient contribution per point\n",
    "    # This is derived from the derivative of the squared error with respect to 'w'\n",
    "    gradient_contributions = errors * x\n",
    "    \n",
    "    # Sum up the gradient contributions for all points and scale by 2/n\n",
    "    # The factor of 2 comes from the derivative of the squared term, and we divide by 'n'\n",
    "    # to get the average gradient across all data points.\n",
    "    # To minimize the cost, we want to change `w` in the opposite direction of the gradient.\n",
    "    gradient = (2.0 / n) * np.sum(gradient_contributions)\n",
    "    \n",
    "    # The calculated gradient indicates the slope of the cost function at the current 'w'\n",
    "    # A positive gradient suggests increasing cost with increasing 'w' and vice versa.\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# Function to plot the current model\n",
    "def plot_current_model(w, x, y, epoch):\n",
    "    plt.scatter(x, y, color='blue')  # Plot the data points\n",
    "    y_pred = w * x\n",
    "    plt.plot(x, y_pred, color='red', linestyle='dotted')  # Plot the line\n",
    "    plt.title(f'Epoch {epoch+1}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradient_descent(x, y, learning_rate=0.01, epochs=10):\n",
    "    w = 0.01  # Start with an arbitrary weight\n",
    "    for epoch in range(epochs):\n",
    "        mse = calculate_mse(w, x, y)\n",
    "        gradient = calculate_gradient(w, x, y)\n",
    "        w -= learning_rate * gradient  # Update the weight\n",
    "\n",
    "        # Print detailed explanation\n",
    "        print(f'Epoch: {epoch+1}:')\n",
    "        print(f'Current weight: {w}')\n",
    "        print(f'(MSE) is the average of the squares of the errors between the predicted y (w*x) and actual y.: {mse}')\n",
    "        print(f'Gradient for single param variable is ordinary derivative of the MSE with respect to weight w. It tells us the direction to move on the w-axis to minimize the MSE {gradient}')\n",
    "        print('The gradient is the partial derivative of the MSE with respect to weight w. It tells us the direction to move on the w-axis to minimize the MSE.')\n",
    "        \n",
    "        # Call the function to plot the current model\n",
    "        plot_current_model(w, x, y, epoch)\n",
    "    \n",
    "    return w\n",
    "\n",
    "# Perform Gradient Descent\n",
    "optimized_w = gradient_descent(x, y, learning_rate=0.01, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Derevative explained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume x and y are numpy arrays containing the data points\n",
    "x = np.array([1, 2, 3, 4, 5, 6])\n",
    "y = np.array([2, 4, 8, 16, 32, 64])\n",
    "n = len(x)\n",
    "\n",
    "def calculate_derivative(w, x, y):\n",
    "    # Step 1: Calculate the predictions based on the current weight\n",
    "    predictions = w * x  # This is the model's guess: y_hat = w * x\n",
    "    \n",
    "    # Step 2: Calculate the errors (differences between predictions and actual values)\n",
    "    errors = predictions - y  # Error for each point: e_i = y_hat_i - y_i\n",
    "    \n",
    "    # Step 3: Calculate the derivative of the square of errors with respect to w\n",
    "    # For each point, this is: d/dw (e_i^2) = 2 * e_i * d/dw (e_i)\n",
    "    # And since e_i = w*x_i - y_i, d/dw (e_i) = x_i\n",
    "    # Thus, the derivative for each point is: 2 * e_i * x_i\n",
    "    derivative_errors = 2 * errors * x  # Derivative for each point\n",
    "    \n",
    "    # Step 4: Average the derivatives across all data points\n",
    "    # This gives us the mean of the gradients from all data points\n",
    "    gradient = np.sum(derivative_errors) / n  # The mean gradient\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Example of using the function with an initial weight\n",
    "initial_w = 0.1\n",
    "gradient_at_initial_w = calculate_derivative(initial_w, x, y)\n",
    "print(f\"The gradient at w = {initial_w} is {gradient_at_initial_w}\")\n",
    "\n",
    "# For visualization purposes, let's plot J(w) against w values\n",
    "weights = np.linspace(-10, 10, 400)  # A range of weight values for plotting\n",
    "costs = np.array([calculate_mse(w, x, y) for w in weights])  # Corresponding MSE values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(weights, costs, label='J(w)')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('J(w)')\n",
    "plt.title('Cost Function J(w) vs. weight w')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
