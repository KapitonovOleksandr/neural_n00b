{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Models\n",
    "\n",
    "\n",
    "# TODO {\n",
    "Background\n",
    "Motivation\n",
    "What problems ResNets solve?\n",
    "Architecture\n",
    "Summary\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "Machine translation, translating an input sentence from one language to another, is a great use case.\n",
    "The representation is quite intuitive: sentences can be regarded as sequences of words.\n",
    "\n",
    "\n",
    "### Seq2Seq\n",
    "\n",
    "Ideally, a model has to understand the input sentence in one language. This is captured in the so-called “encoder”. \n",
    "We need to convert the meaning into another language, so let’s call this model decoder.\n",
    "\n",
    "<img src=\"./img/transformers/Seq2Seq.png\" alt=\"seg2seq\" width=\"700\"/>\n",
    "\n",
    "The goal is to transform an input sequence (source) to a new one (target). The two sequences can be of the same or arbitrary length.\n",
    "\n",
    "The reason is simple: we liked to treat sequences sequentially. Sounds obvious and optimal? Attention mechanisms, and eventually transformers, proved that it was not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comprehensive view of encoder and decoder\n",
    "\n",
    "Let’s suppose that the encoder and decoder are stacked RNN/LSTM cells. z can be regarded as a compressed format of the input.\n",
    "\n",
    "<img src=\"./img/transformers/s2sLSTM.png\" alt=\"s2sLSTM\" width=\"400\"/>\n",
    "\n",
    "The decoder receives the context vector z and generates the output sequence\n",
    "\n",
    "We can think of the input sequence as the representation of a sentence in English and the output as the same sentence in French.\n",
    "\n",
    "\n",
    "<img src=\"./img/transformers/s2s_rnn.png\" alt=\"s2s_rnn\" width=\"400\"/>\n",
    "<img src=\"./img/transformers/s2s_limitations.png\" alt=\"s2s_limitations\" width=\"400\"/>\n",
    "\n",
    "In fact, RNN-based architectures used to work very well, especially with LSTM components.\n",
    "\n",
    "The problem? Only for small sequences (<20 timesteps). Visually:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The limitations of RNNs\n",
    "\n",
    "The main issue is that the intermediate representation z cannot encode information from all the input timesteps. \n",
    "\n",
    "This is commonly known as the **bottleneck problem.**\n",
    "The vector z needs to capture all the information about the source sentence.\n",
    "\n",
    "RNNs tend to forget information from timesteps that are far behind.\n",
    "\n",
    "\n",
    "For example, 97 words sentance:\n",
    "\n",
    "<img src=\"./img/transformers/97_words_sentance.png\" alt=\"derivative\" width=\"700\"/>\n",
    "\n",
    "**\"bliend man\"** is a key for the understanding of the text. The vector z will be unable to compress the information of the first few words as well as the 97th word.\n",
    "\n",
    "Eventually, the system pays more attention to the last parts of the sequence. **This is not usually the optimal way to approach a sequence task**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Attention was born in order to address the limitations of Seq2Seq models.\n",
    "\n",
    "The core idea is that the context vector z should have access to **all parts** of the input sequence instead of just the last one.\n",
    "\n",
    "We can look at all the different words at the same time and learn to **“pay attention“** to the correct ones depending on the task at hand.\n",
    "\n",
    "Attention is simply a notion of memory gained from attending at multiple inputs through time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention as an alignment between words\n",
    "\n",
    "The attention score describes the relationship between the two states and captures how “aligned” they are.\n",
    "\n",
    "Many different ideas to compute that score. The simplest one computes attention as the dot product between the two states **yi−1h**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines an attention mechanism, which is a component in neural networks \n",
    "    that helps the model to dynamically focus on certain parts of the input.\n",
    "    It inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, y_dim: int, h_dim: int):\n",
    "        \"\"\"\n",
    "        The constructor for the Attention class.\n",
    "        \n",
    "        Args:\n",
    "        y_dim (int): The dimension of the input tensor y.\n",
    "        h_dim (int): The dimension of the input tensor h.\n",
    "        \"\"\"\n",
    "        super().__init__()  # Initializes the base class nn.Module.\n",
    "        \n",
    "        # Store the dimensions of y and h in instance variables for later use.\n",
    "        self.y_dim = y_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        # Define a learnable weight matrix W as a parameter of the module.\n",
    "        # The shape of W is (y_dim, h_dim), allowing it to transform a vector from y_dim to h_dim.\n",
    "        self.W = nn.Parameter(torch.FloatTensor(self.y_dim, self.h_dim))\n",
    "\n",
    "    def forward(self, y: torch.Tensor, h: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The forward pass of the Attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "        y (torch.Tensor): The input tensor y with shape (batch_size, y_dim).\n",
    "        h (torch.Tensor): The input tensor h with shape (batch_size, h_dim).\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: The result of applying the attention mechanism to the input tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the attention scores.\n",
    "        # This is done by matrix-multiplying the input y with the weight matrix W,\n",
    "        # and then with the transpose of h.\n",
    "        # The result is a score matrix of shape (batch_size, batch_size), \n",
    "        # representing the attention score between each pair of y and h elements.\n",
    "        score = torch.matmul(torch.matmul(y, self.W), h.T)\n",
    "\n",
    "        # Apply the softmax function to the scores along dimension 0.\n",
    "        # This normalizes the scores so that they sum up to 1, \n",
    "        # making them interpretable as probabilities.\n",
    "        z = F.softmax(score, dim=0)\n",
    "\n",
    "        # Multiply the normalized scores with the input h.\n",
    "        # This step effectively computes a weighted sum of the h vectors, \n",
    "        # with the weights given by the attention scores.\n",
    "        # The result is a tensor of shape (batch_size, h_dim), \n",
    "        # containing the attended features.\n",
    "        return torch.matmul(z, h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/transformers/self_attention.png\" alt=\"derivative\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Architecture Overview \n",
    "\n",
    "### 1. Sets and tokenization\n",
    "\n",
    "<img src=\"./img/transformers/sets_and_tokenization.png\" alt=\"sets_and_tokenization\" width=\"700\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nn_n00b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
