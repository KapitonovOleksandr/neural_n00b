{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.features =16\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=3072, out_features=128)\n",
    "        self.enc2 = nn.Linear(in_features=128, out_features=self.features * 2)\n",
    "\n",
    "        # decoder\n",
    "        self.dec1 = nn.Linear(in_features=self.features, out_features=128)\n",
    "        self.dec2 = nn.Linear(in_features=128, out_features=3072)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = self.enc2(x).view(-1, 2, self.features)\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x[:, 0, :]  # the first feature values as mean\n",
    "        log_var = x[:, 1, :]  # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        reconstruction = torch.sigmoid(self.dec2(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 14 (284544665.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    bce_loss = nn.BCELoss(reduction='sum')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 14\n"
     ]
    }
   ],
   "source": [
    "    # 1. In simple terms, a variational autoencoder is a probabilistic version of autoencoders.\n",
    "    # 2. VAE to be able to sample from the latent vector (z) space to generate new data, which is not possible with vanilla autoencoders.\n",
    "    # 3. Each latent variable z that is generated from the input will now represent a probability distribution \n",
    "    #    (or what we call the posterior distribution denoted as p(z∣x))\n",
    "    # 4. All we need to do is find the posterior p(z∣x) or solve the inference problem.\n",
    "    # 5. In fact, the encoder will try to approximate the posterior by computing another distribution q(z∣x) known as the variational posterior.\n",
    "    #!6 Probability distribution is fully characterized by its parameters. In the case of the Gaussian, these are the mean μ and the standard deviation σ\n",
    "    #!7. Decoder will receive the distribution parameters and try to reconstruct the input x. However you cannot backpropagate through a sampling operation.\n",
    "    #!8. \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "    \n",
    "#Train a variational autoencoder \n",
    "def elbo(reconstructed, input, mu, logvar):    \n",
    "    bce_loss = nn.BCELoss(reduction='sum')\n",
    "    BCE = bce_loss(reconstructed, input)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparameterization trick\n",
    "def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        In order to generate samples from the encoder and pass them to the decoder, we also need to utilize the reparameterization trick. \n",
    "        Don’t forget that we need to be able to run the backward pass during training.\n",
    "        \n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)  # standard deviation\n",
    "        eps = torch.randn_like(std)  # generate sample of the same size\n",
    "        sample = mu + (eps * std)  # sampling as if coming from the input space\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of loss terms\n",
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the\n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model,training_data):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, data in enumerate(training_data, 0):\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, mu, logvar = model(inputs)\n",
    "            bce_loss = criterion(reconstruction, inputs)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nn_n00b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
