{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST Dataset\n",
    "\n",
    "The dataset is designed for machine learning classification tasks and contains in total 60 000 training and 10 000 test images (gray scale) with each 28x28 pixel. Each training and test case is associated with one of ten labels (0–9).\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/Fashion_MNIST.png\" alt=\"nearby_objects\" width=\"800\"/>\n",
    "\n",
    "# NNs Architectures for the imgs classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet \n",
    "\n",
    "\n",
    "Background\n",
    "Motivation\n",
    "What problems AlexNet solve?\n",
    "Architecture\n",
    "Summary\n",
    "\n",
    "\n",
    "\n",
    "- This was the first architecture that used GPU to boost the training performance.\n",
    "\n",
    "- AlexNet consists of 5 convolution layers, 3 max-pooling layers, 2 Normalized layers, 2 fully connected layers and 1 SoftMax layer.\n",
    "\n",
    "- Each convolution layer consists of a convolution filter and a non-linear activation function called “ReLU”.\n",
    "\n",
    "- The pooling layers are used to perform the max-pooling function and the input size is fixed due to the presence of fully connected layers. \n",
    "\n",
    "- input size is mentioned at most of the places as 224x224x3 but due to some padding which happens it works out to be 227x227x3. Above all this AlexNet has over 60 million parameters.\n",
    "\n",
    "#### Key Features:\n",
    "\n",
    "- ‘ReLU’ is used as an activation function rather than ‘tanh’\n",
    "- Batch size of 128\n",
    "- SGD Momentum is used as a learning algorithm\n",
    "- Data Augmentation is been carried out like flipping, jittering, cropping, colour normalization, etc.\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/AlexNet_Architecture.png\" alt=\"nearby_objects\" width=\"800\"/>\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/AlexNet_architecture_layers.png\" alt=\"nearby_objects\" width=\"800\"/>\n",
    "\n",
    "- **Input data:** This is the raw input that feeds into the network. For AlexNet, the input size is typically a 227x227 pixel image with 3 channels (corresponding to RGB color channels).\n",
    "\n",
    "- **Conv1:** (Convolutional Layer 1): This layer performs convolution operations that involve filtering the input image with learned kernels to create a feature map. The dimensions 55x55x96 indicate that this layer produces 96 separate feature maps, each of size 55x55. This layer detects simple features like edges and corners.\n",
    "\n",
    "- **Conv2:** (Convolutional Layer 2): Another convolutional layer that takes the output from Conv1 and applies a different set of filters to detect more complex features. It outputs 256 feature maps, each of size 27x27.\n",
    "\n",
    "- **Conv3:** (Convolutional Layer 3): This is a deeper convolutional layer in the network, which works on even more abstract features. With 384 feature maps, each 13x13, this layer allows the network to begin understanding more complex structures within the image.\n",
    "\n",
    "- **Conv4:** (Convolutional Layer 4): Similar to Conv3, but typically the filters in this layer are more specialized to detect high-level features. It outputs the same number of feature maps (384) but with a different set of filters.\n",
    "\n",
    "- **Conv5:** (Convolutional Layer 5): This is usually the last convolutional layer and outputs 256 feature maps of size 13x13. It is directly connected to the fully connected layers and often captures the highest-level features.\n",
    "\n",
    "beginning the process of classifying the image based on the features extracted by the convolutional layers. Each neuron in this layer is connected to all the activations in the previous layer, thus it's called fully connected.\n",
    "\n",
    "- **FC6:** (Fully Connected Layer 6): A dense layer with 4096 units that takes all the feature maps from Conv5 and flattens them into a single vector. This layer is responsible for\n",
    "\n",
    "- **FC7:** (Fully Connected Layer 7): This is another dense layer with 4096 units, which continues the classification process. It takes the output from FC6 and further processes it, potentially learning even more abstract representations of the image features.\n",
    "\n",
    "FC6 and FC7 have the same size of 4096 units to provide sufficient capacity for complex feature representation and to allow for a deeper hierarchy of learned features, which was empirically found to be effective for image classification tasks.\n",
    "\n",
    "- **FC8:** (Fully Connected Layer 8): This is the final fully connected layer and usually represents the output layer of the network. In the case of AlexNet, it typically has 1000 units, each corresponding to a different class in a classification task. The network will output a probability distribution across these 1000 classes, indicating how likely the input image belongs to each class. Typically connects to a softmax or logistic regression output layer\n",
    "\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/AlexNet_architecture_layers2.png\" alt=\"nearby_objects\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 21:21:57,995 - INFO - Epoch 1, Loss: 2.302598403930664\n",
      "2024-01-15 21:22:39,325 - INFO - Epoch 2, Loss: 2.302577648162842\n",
      "2024-01-15 21:23:24,046 - INFO - Epoch 3, Loss: 2.3025367193222044\n",
      "2024-01-15 21:24:11,448 - INFO - Epoch 4, Loss: 2.302519413948059\n",
      "2024-01-15 21:24:56,798 - INFO - Epoch 5, Loss: 2.302498456954956\n",
      "2024-01-15 21:25:47,228 - INFO - Epoch 6, Loss: 2.3024621686935425\n",
      "2024-01-15 21:26:28,027 - INFO - Epoch 7, Loss: 2.3024250869750977\n",
      "2024-01-15 21:27:06,881 - INFO - Epoch 8, Loss: 2.3024126749038696\n",
      "2024-01-15 21:27:47,419 - INFO - Epoch 9, Loss: 2.3023795166015626\n",
      "2024-01-15 21:28:28,214 - INFO - Epoch 10, Loss: 2.302334545135498\n",
      "2024-01-15 21:28:28,215 - INFO - Finished Training\n",
      "2024-01-15 21:28:31,273 - INFO - Accuracy of the network on test images: 11 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "# Define the AlexNet Model\n",
    "class AlexNet(nn.Module):        \n",
    "    \"\"\"_summary_\n",
    "    # Input size is 227x227x3 as per the architecture   \n",
    "    nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),  # First Convolutional Layer\n",
    "    \n",
    "    1 First parameter (in_channels=3): This specifies the number of channels in the input image. \n",
    "        For RGB images, this is 3, which means there are three channels for red, green, and blue.\n",
    "\n",
    "    2 Second parameter (out_channels=96): This is the number of filters that will be applied to the input image. \n",
    "        It also represents the number of feature maps that will be produced by the convolution. In this case, \n",
    "        96 filters will generate 96 separate feature maps, meaning the layer will output a tensor with 96 channels.\n",
    "\n",
    "    3 kernel_size=11: This defines the size of the filter (also known as the kernel) that will be used to perform the convolution. \n",
    "        In this case, the filter will be 11 pixels by 11 pixels. \n",
    "        The kernel size affects the area of the input image that is used to compute each element of the output feature map.\n",
    "\n",
    "    4 stride=4: The stride is the number of pixels by which the filter moves across the input image. \n",
    "        A stride of 4 means the filter jumps 4 pixels at a time as it moves across the image. \n",
    "        This results in downsampling the output feature map by a factor of 4, making it smaller than the input image.\n",
    "\n",
    "    5 padding=0: Padding adds zeros to the border of the input image. This is used to control the spatial size of the output feature maps.\n",
    "        A padding of 0 means no padding is applied, and the spatial dimensions of the output will be reduced compared to the input dimensions. \n",
    "        Padding can also be used to ensure that the output feature map has the same spatial dimensions as the input image (called 'same' padding),\n",
    "        but in this case, since the padding is 0, the output will be smaller.    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):  # Default is 10 classes as per the architecture provided\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Input size is 227x227x1 as per the architecture One channel == Fashion Dataset                                   \n",
    "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),  # First Convolutional Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2),  # Second Convolutional Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),  # Third Convolutional Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),  # Fourth Convolutional Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),  # Fifth Convolutional Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))  # Adaptive pooling to make it size-independent\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            # CF 6 \n",
    "            nn.Linear(256 * 6 * 6, 4096),  # First Fully Connected Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            # CF 7\n",
    "            nn.Linear(4096, 4096),  # Second Fully Connected Layer\n",
    "            nn.ReLU(inplace=True),\n",
    "            # FC 8 input features from the left hand side hiddel layer 4096, uotputs classes -> FashionDataset = num_classes \n",
    "            nn.Linear(4096, num_classes),  # \n",
    "            nn.Softmax(dim=1)  # Softmax activation as per the architecture\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset():\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "    #VGG16 architecture is designed for images of size 224x224\n",
    "    transforms.Resize((327, 327)),  # Resize the image to 224x224\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    full_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    full_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_indices = np.arange(1000) \n",
    "    test_indices = np.arange(100)  \n",
    "    trainset = Subset(full_trainset, train_indices)\n",
    "    testset = Subset(full_testset, test_indices)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "\n",
    "trainloader, testloader = load_dataset()\n",
    "\n",
    "# Define training process\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        logger.info(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
    "    logger.info('Finished Training')\n",
    "\n",
    "# Define testing process\n",
    "def test_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    logger.info('Accuracy of the network on test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Initialize the AlexNet model\n",
    "model = AlexNet()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, trainloader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Test the model\n",
    "test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet (Visual Geometry Group (VGG))\n",
    "\n",
    "\n",
    "- The VGGNet, developed by the Visual Geometry Group (VGG) at the University of Oxford, is primarily used for image classification.\n",
    "\n",
    "- Visual Geometry Group (VGG) was created to improve the model's performance by increasing the depth of such CNNs.\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/VGG1.png\" alt=\"nearby_objects\" width=\"600\"/>\n",
    "\n",
    "**Input:** The input layer typically receives the raw pixel data of the image, which in this case seems to be a 224x224 pixel image with 64 channels.\n",
    "\n",
    "**Convolutional Layers (Conv):** These layers perform convolution operations to extract features from the input image. They\n",
    "\n",
    "use filters (or kernels) to capture local patterns such as edges, textures, and gradients within the image. Each convolutional layer applies a number of these filters to the image or to the output of previous layers, and the depth of each convolutional block (e.g., depth-64, depth-128) represents the number of unique filters used, producing a corresponding number of feature maps.\n",
    "\n",
    "- conv1_1, conv1_2: These are the first convolutional layers with a depth of 64, using 3x3 filters to capture basic patterns.\n",
    "\n",
    "- conv2_1, conv2_2: Increase the depth to 128, allowing the network to capture more complex patterns.\n",
    "\n",
    "- conv3_1 to conv3_4: The depth is further increased to 256. More layers mean the network can build more complex features from simpler ones captured in earlier layers.\n",
    "\n",
    "- conv4_1 to conv4_4: Similar to conv3, but with a depth of 512, allowing even more complex features to be captured.\n",
    "\n",
    "- conv5_1 to conv5_4: These are the final convolutional layers before the network transitions to fully connected\n",
    "layers. They also have a depth of 512 and continue to refine the high-level features extracted from the image.\n",
    "\n",
    "**Pooling Layers (Maxpool):** These layers follow some of the convolutional blocks and perform downsampling to reduce the spatial dimensions of the feature maps. This helps to reduce the amount of computation needed and also makes the features somewhat invariant to small translations of the input image. Max pooling layers typically select the maximum value from a small neighborhood (like 2x2 pixels) to represent the region.\n",
    "\n",
    "\n",
    "**Fully Connected Layers (FC):** After several layers of convolutions and pooling, the network uses fully connected layers to perform classification based on the features extracted. Each neuron in a fully connected layer has connections to all activations in the previous layer. In this network:\n",
    "\n",
    "\n",
    "**FC1:** The first fully connected layer has 4096 neurons, which take the flattened output of the last max pooling layer as input.\n",
    "\n",
    "**FC2:** The second fully connected layer also has 4096 neurons and continues the process of interpreting the extracted features.\n",
    "Size=1000 (Softmax): The final layer has 1000 neurons, each corresponding to a class in the dataset (assuming this network was designed for a 1000-class classification task like ImageNet). The softmax function is applied to this layer to obtain a probability distribution over the 1000 classes.\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/VGG2.png\" alt=\"nearby_objects\" width=\"800\"/>\n",
    "\n",
    "<img src=\"./img/comp_vision/img_classification/VGG3.png\" alt=\"nearby_objects\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 13:05:52,728 - INFO - VGGNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "2024-01-11 13:05:56,272 - INFO - Epoch: 1, Batch: 1, Avg. Loss: 0.0346\n",
      "2024-01-11 13:06:13,393 - INFO - Epoch: 1, Batch: 16, Avg. Loss: 263878857227601224663040.0000\n",
      "2024-01-11 13:06:24,862 - INFO - Epoch: 2, Batch: 1, Avg. Loss: 10059832360.9600\n",
      "2024-01-11 13:06:40,768 - INFO - Epoch: 2, Batch: 16, Avg. Loss: 70672587279.2700\n",
      "2024-01-11 13:06:52,418 - INFO - Epoch: 3, Batch: 1, Avg. Loss: 31731.3725\n",
      "2024-01-11 13:07:06,558 - INFO - Epoch: 3, Batch: 16, Avg. Loss: 156448.4878\n",
      "2024-01-11 13:07:17,920 - INFO - Epoch: 4, Batch: 1, Avg. Loss: 3278.4453\n",
      "2024-01-11 13:07:32,241 - INFO - Epoch: 4, Batch: 16, Avg. Loss: 24610.8443\n",
      "2024-01-11 13:07:43,936 - INFO - Epoch: 5, Batch: 1, Avg. Loss: 148.4059\n",
      "2024-01-11 13:07:58,305 - INFO - Epoch: 5, Batch: 16, Avg. Loss: 848.3363\n",
      "2024-01-11 13:08:09,804 - INFO - Epoch: 6, Batch: 1, Avg. Loss: 28.6230\n",
      "2024-01-11 13:08:23,887 - INFO - Epoch: 6, Batch: 16, Avg. Loss: 230.4523\n",
      "2024-01-11 13:08:35,437 - INFO - Epoch: 7, Batch: 1, Avg. Loss: 8.2363\n",
      "2024-01-11 13:08:50,159 - INFO - Epoch: 7, Batch: 16, Avg. Loss: 75.2654\n",
      "2024-01-11 13:09:01,607 - INFO - Epoch: 8, Batch: 1, Avg. Loss: 1.2637\n",
      "2024-01-11 13:09:15,756 - INFO - Epoch: 8, Batch: 16, Avg. Loss: 9.4269\n",
      "2024-01-11 13:09:27,309 - INFO - Epoch: 9, Batch: 1, Avg. Loss: 0.0761\n",
      "2024-01-11 13:09:41,715 - INFO - Epoch: 9, Batch: 16, Avg. Loss: 1.7582\n",
      "2024-01-11 13:09:53,126 - INFO - Epoch: 10, Batch: 1, Avg. Loss: 0.0490\n",
      "2024-01-11 13:10:07,330 - INFO - Epoch: 10, Batch: 16, Avg. Loss: 0.6582\n",
      "2024-01-11 13:10:18,573 - INFO - Accuracy of the network on test images: 10.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import logging\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "# Set SSL certificate\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# Setup logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset():\n",
    "    transform = transforms.Compose([\n",
    "    #VGG16 architecture is designed for images of size 224x224\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    full_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    full_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_indices = np.arange(100) \n",
    "    test_indices = np.arange(10)  \n",
    "    trainset = Subset(full_trainset, train_indices)\n",
    "    testset = Subset(full_testset, test_indices)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define the VGGNet class, which inherits from nn.Module, the base class for all neural network modules in PyTorch.\n",
    "class VGGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the VGGNet architecture for image classification.\n",
    "\n",
    "    VGGNet is characterized by its simplicity, using only 3x3 convolutional layers stacked on top of each other in increasing depth. \n",
    "    Reducing volume size is handled by max pooling. Two fully connected layers, each with 4096 nodes are then followed by a softmax \n",
    "    classifier (output layer).\n",
    "\n",
    "    There are several versions of VGGNet. This one seems to be VGG16, which has 16 layers that have weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=1000):\n",
    "        \"\"\"\n",
    "        Initialize the VGGNet model.\n",
    "        \n",
    "        Parameters:\n",
    "            num_classes (int): number of classes for the final softmax layer (default is 1000 for ImageNet).\n",
    "        \"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers in the feature extractor part of the VGGNet\n",
    "        self.features = nn.Sequential(\n",
    "            # conv1: two convolutional layers with 64 output channels each\n",
    "            #nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            # Adjust the first conv layer to take in 1-channel images.\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),   \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # conv2: two convolutional layers with 128 output channels each\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # conv3: three convolutional layers with 256 output channels each\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # conv4: three convolutional layers with 512 output channels each\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # conv5: three convolutional layers with 512 output channels each\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Define the fully connected layers in the classifier part of the VGGNet\n",
    "        self.classifier = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            nn.Linear(in_features=7 * 7 * 512, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            # Second fully connected layer\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            # Final layer with output size = number of classes\n",
    "            nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    # Define the forward pass of the network.\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers.\n",
    "        x = self.features(x)\n",
    "        # Flatten the output of the conv layers to fit the fully connected layers.\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass the flattened output through the fully connected layers.\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(model, trainloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 15 == 0:\n",
    "                logging.info(f'Epoch: {epoch + 1}, Batch: {i + 1}, Avg. Loss: {running_loss / 200:.4f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "def test(model, testloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    logging.info(f'Accuracy of the network on test images: {100 * correct / total}%')\n",
    "\n",
    "# Main execution\n",
    "trainloader, testloader = load_dataset()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the VGGNet class with 10 output classes (default).\n",
    "# Assuming the use of Fashion-MNIST dataset where images are 28x28 in size.\n",
    "\n",
    "#TODO implementation from scratch\n",
    "model = VGGNet().to(device)\n",
    "\n",
    "#TODO compare with VGG model from torch\n",
    "#import torchvision.models as models\n",
    "#num_classes = 10\n",
    "#model = models.vgg16(pretrained=True)\n",
    "#model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1) # Modify the first convolutional layer\n",
    "#model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, num_classes) # Modify the classifier for 10 output classes\n",
    "\n",
    "\n",
    "# Show the architecture of the model.\n",
    "logging.info(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 10  # Define the number of epochs\n",
    "train(model, trainloader, criterion, optimizer, device)\n",
    "test(model, testloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R-CNN (Region-based Convolutional Neural Network):\n",
    "\n",
    "- The original R-CNN model proposes a method where selective search is used to generate region proposals. These regions are then fed into a CNN to extract features, which are subsequently classified by a set of SVMs (Support Vector Machines).\n",
    "\n",
    "- It's computationally expensive due to the need to process multiple regions per image separately.\n",
    "\n",
    "- **SVM Training:** You then train an SVM classifier using these feature vectors as input. The SVM will learn a hyperplane that best separates the feature vectors into different classes based on the training data you provide. Each class in your training set should have a corresponding label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting feature extraction for training data\n",
      "INFO:__main__:Starting SVM training\n",
      "INFO:__main__:SVM training completed\n",
      "INFO:__main__:Starting SVM testing\n",
      "INFO:__main__:Accuracy of the SVM classifier on the 100 test images: 76.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load and preprocess a subset of the Fashion-MNIST dataset.\n",
    "    Returns DataLoader objects for the subsets of training (100 instances) \n",
    "    and testing datasets (10 instances).\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    # Load the full Fashion-MNIST datasets\n",
    "    full_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    full_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Define indices for the subsets\n",
    "    train_indices = np.arange(1000) \n",
    "    test_indices = np.arange(100)  \n",
    "\n",
    "    # Create subset datasets\n",
    "    trainset = Subset(full_trainset, train_indices)\n",
    "    testset = Subset(full_testset, test_indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "# Define the R-CNN Model\n",
    "class FashionRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the FashionRCNN model.\n",
    "        The model consists of a simple CNN for feature extraction.\n",
    "        \"\"\"\n",
    "        super(FashionRCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        Applies the feature extraction layers.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        return x.view(x.size(0), -1)  # Flatten the output\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract features from the input batch.\n",
    "        This function is used for feature extraction for SVM training.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.features(x)\n",
    "        return x.view(x.size(0), -1).cpu().numpy()\n",
    "\n",
    "# Main function to execute the training and testing\n",
    "def main():\n",
    "    trainloader, testloader = load_dataset()\n",
    "    net = FashionRCNN()\n",
    "\n",
    "    # Extracting features from training data\n",
    "    logger.info('Starting feature extraction for training data')\n",
    "    features = []\n",
    "    labels = []\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, label = data\n",
    "        feature = net.extract_features(inputs)\n",
    "        features.extend(feature)\n",
    "        labels.extend(label.numpy())\n",
    "\n",
    "    # SVM classifier training\n",
    "    logger.info('Starting SVM training')\n",
    "    svm_classifier = make_pipeline(StandardScaler(), svm.SVC(gamma='auto'))\n",
    "    svm_classifier.fit(features, labels)\n",
    "    logger.info('SVM training completed')\n",
    "\n",
    "    # Testing with SVM classifier\n",
    "    logger.info('Starting SVM testing')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        images, label = data\n",
    "        feature = net.extract_features(images)\n",
    "        predicted = svm_classifier.predict(feature)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label.numpy()).sum().item()\n",
    "\n",
    "    logger.info(f'Accuracy of the SVM classifier on the 100 test images: {100 * correct / total}%')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nn_n00b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
